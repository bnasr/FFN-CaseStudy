# -*- coding: utf-8 -*-
"""FFN Case Study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1le1emKWg3kDgibay0SIrE6v0-VB460UZ

# DS Growth Case Study

## Context: 
A user is looking for loans on Google and enters a search query. Depending on how related the search query is to FFN business, we choose to bid for the query with the goal of maximizing the user impressions using Google Adwords (Google actually showing the ad to the user when they execute the search). This would lead to more clicks on ads, and potentially more signups, activations, and revenue. Here activation is defined as a user successfully contacted by FFN call center.

## Questions:

1.	Using the dataset attached ‘clicks-activation_v2.csv’, build a model to predict the probability of activation per click.

2.	Please create a presentation using the following guidelines:
  -	Describe the workflow steps (explicitly setting up the optimization problem and choice of objective function) and your hypothesis.
  -	Evaluate the performance of the model (pre-production)?
  -	What are some of the shortcomings of this approach?
  -	What are some other features you might need to come up with a better model?

3.	What does the engineering architecture to productionize one of the solutions looks like? What trade-offs would you use to productionize promising algorithms faster?

4.	Discuss how will you analyze the business performance of the algorithm once in production using metrics such as marketing ROI etc.?

Each of the four parts above carry equal weightage in evaluation. We don’t think you need to spend more than 3 hours on the overall exercise (please feel free to take more time if you want). If you have questions please contact: azajic@freedomfinancialnetwork.com

## Data Dictionary:

1.	created_date - date on which the click was recorded
2.	location_in_query - city name that appeared in search query
3.	platform - platform such as web/mobile
4.	campaign_state - US state where the ad campaign was run
5.	in_city - whether the ad campaign was run in a city or not
6.	is_prime - whether the word fico prime appeared in the search query
7.	is_hardship - whether the word hardship appeared in the search query
8.	category_debt_type - type of debt solution that appeared in the search query
9.	[TARGET] is_activated – whether the user talked to FFN call center or not

# Loading required modules
"""

import numpy as np
import pandas as pd

import tensorflow as tf

from tensorflow import feature_column
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

"""# Loading the data
Let's load the data file as Pandas DateFrame:
"""

#loading the data
clicks = pd.read_csv('drive/My Drive/ffn/data/clicks-activation_v2.csv')

#creating a new column for time as integer
clicks['time_index'] = pd.to_datetime(clicks['created_date']).astype(int)

#printing data types for each columns
print(clicks.dtypes, '\n')

#printing info summary
print(clicks.info(), '\n')

#printing first few rows
clicks.head()

clicks['is_activated'] = np.where(clicks['is_activated']==True, 0, 1)

clicks.is_activated.value_counts()

"""As shown above there are missing values in different columns. We can take a closer look at the statistics  using the below command."""

print(clicks.isnull().sum())

"""# Incorporating Date and Time
I use time/date as two main features: 1) as a categorical variable 2) as a continuous normalized number
"""

scaler = MinMaxScaler()
clicks[['time_index']] = scaler.fit_transform(clicks[['time_index']])

"""# Imputation
There are missing values in three categorical columns that should be imputed: `location_in_query`, `campaign_state`, and `category_debt_type`.
"""

states = ['al', 'ak', 'az', 'ar', 'ca', 'co', 'ct', 'dc', 'de', 'fl', 'ga', 'hi', 
          'id', 'il', 'in', 'ia', 'ks', 'ky', 'la', 'me', 'md', 'ma', 'mi', 'mn',
          'ms', 'mo', 'mt', 'ne', 'nv', 'nh', 'nj', 'nm', 'ny', 'nc', 'nd', 'oh',
          'ok', 'or', 'pa', 'ri', 'sc', 'sd', 'tn', 'tx', 'ut', 'vt', 'va', 'wa', 
          'wv', 'wi', 'wy']

# also imputing states where the value is a number
clicks.loc[~clicks['campaign_state'].isin(states),'campaign_state'] = np.nan

#imputing campaign_state with the most frequent state
clicks['campaign_state'].fillna(clicks['campaign_state'].mode()[0], inplace = True)

#imputing location_in_query with the unknown_location
clicks['location_in_query'].fillna('unknown_location', inplace = True)

#imputing category_debt_type with the most frequent value
clicks['category_debt_type'].fillna(clicks['category_debt_type'].mode()[0], inplace = True)

print(clicks.isnull().sum())

"""# Balance the data
As the training data is imbalanced, we need to adjust number of positive vs negaive labels.
"""

from sklearn.utils import resample

data = clicks.copy()

#check the length of each class
print('oringal data class size:\n',data.is_activated.value_counts(), '\n')

#creating subsets
data_not_activated = data[np.array(data.is_activated) == 0]
data_activated = data[np.array(data.is_activated) == 1]

#upsampling of the minorty class
n_samples = len(data_activated)

#downsampling of the minorty class
#n_samples = len(data_not_activated)

data_not_activated_resampled = resample(data_not_activated, n_samples = n_samples, random_state = 0)
data_activated_resampled = resample(data_activated, n_samples = n_samples, random_state = 0)

#merged data back together
data_resampled = pd.concat([data_not_activated_resampled, data_activated_resampled])

#shuffle the data
data_resampled = data_resampled.sample(frac = 1) 

#check the final stats
print('resampled data class size:\n', data_resampled.is_activated.value_counts())

"""# Split the Data 
One important step is always to split the data into train, validation and test. We accomplish this by the two times running the train_test_split function from sklearn.
"""

# splitting the data

# the test data will be used only after the model is trained
train_valid, test = train_test_split(data_resampled, test_size = 0.2, shuffle = True) 

# the valid data will be used during the training but not for the training, only for evaluations per epoch
train, valid = train_test_split(train_valid, test_size = 0.25, shuffle = True) 

#printig their length
print('training data size:', len(train))
print('validation data size:', len(valid))
print('testing data size:', len(test))

"""# Creating TensorFlow Ready Datasets
To prepare the input data for the TensorFlow data pipeline, we need to put to in sliced batches.

---
"""

def create_dataset(data, batch_size=1024):
  df = data.copy()
  labels = df.pop('is_activated')
  
  dataset = tf.data.Dataset.from_tensor_slices((dict(df), labels))

  # shuffle the dataset
  dataset = dataset.shuffle(buffer_size=len(df))

  dataset = dataset.batch(batch_size)
  return dataset

train_data = create_dataset(train)
val_data = create_dataset(valid)
test_data = create_dataset(test)

"""# Feature Columns
To create the list of what feature to put in the model, we use the `feature_column` module from `TensorFlow`.
"""

for i in clicks.columns:
  print(i, len(clicks[i].unique()), clicks[i].dtype)

clicks.platform.unique()

from tensorflow import feature_column

feature_columns = []

#first numerical columns
for col_name in ['in_city', 'is_prime', 'is_hardship', 'time_index']:
  feature_columns.append(feature_column.numeric_column(col_name))

# indicator columns
indicator_column_names = ['platform', 'category_debt_type']
for col_name in indicator_column_names:
  categorical_column = feature_column.categorical_column_with_vocabulary_list(col_name, clicks[col_name].unique())
  indicator_column = feature_column.indicator_column(categorical_column)
  feature_columns.append(indicator_column)

# embedding cateogorical columns
embeding_columns = ['created_date', 'campaign_state']
for col_name in embeding_columns:
  vocab = clicks[col_name].unique()
  categorical_column = feature_column.categorical_column_with_vocabulary_list(col_name, vocab)
  column_embedding = feature_column.embedding_column(categorical_column, dimension= 10)#int(len(vocab) *.3))
  feature_columns.append(column_embedding)
  
# crossed columns
campaign_state = feature_column.categorical_column_with_vocabulary_list('campaign_state', clicks.campaign_state.unique())
location_in_query = feature_column.categorical_column_with_vocabulary_list('location_in_query', clicks.location_in_query.unique())
state_location = feature_column.crossed_column([campaign_state, location_in_query], hash_bucket_size=500)
feature_columns.append(feature_column.indicator_column(state_location))

"""# Model"""

model = tf.keras.Sequential()

model.add(layers.DenseFeatures(feature_columns))

model.add(layers.Dense(1024, activation='relu'))
model.add(layers.Dropout(0.5))

model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dropout(0.5))

model.add(layers.Dense(1, activation='sigmoid'))

# I used four main metrics for our binary classification.
metrics = [
      tf.keras.metrics.BinaryAccuracy(name='accuracy'),
      tf.keras.metrics.AUC(name='auc'),
      tf.keras.metrics.Precision(name='precision'),
      tf.keras.metrics.Recall(name='recall'),
]

metrics_names =  ['loss','accuracy', 'auc', 'precision', 'recall']

#model compile
model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate= 0.001),
              loss = tf.keras.losses.BinaryCrossentropy(),
              metrics = metrics)

class_weight = {1: 1,
                0: 0.75}

history = model.fit(train_data,
                    validation_data = val_data, 
                    class_weight = class_weight,
                    epochs=20)

print('training evaluation:', model.evaluate(train_data))
print('test evaluation:', model.evaluate(test_data))

model.save('drive/My Drive/ffn/model.tfm')

"""## Model performance evaluation"""

# load plotting modules
import matplotlib
import matplotlib.pyplot as plt

# setting fonts for axes
matplotlib.rcParams['figure.figsize'] = [24, 8] # width and height of figures

font = {'family' : 'DejaVu Sans', # font
        'weight' : 'bold',
        'size'   : 16}
matplotlib.rc('font', **font)

colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

for n, metric in enumerate(metrics_names[0:3]):
    name = metric.replace("_"," ").upper()
    plt.subplot(1,3,n+1)
    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='train')
    plt.plot(history.epoch, history.history['val_'+metric], color=colors[1], linestyle="-", label='valid')
    plt.xlabel('epoch')
    plt.ylabel(name)
    plt.ylim([0.4,0.85])
    if metric == 'recall': plt.legend()